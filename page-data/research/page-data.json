{"componentChunkName":"component---src-pages-research-js","path":"/research/","result":{"data":{"site":{"siteMetadata":{"title":"Kunjal Panchal"}},"allMarkdownRemark":{"edges":[{"node":{"excerpt":"","fields":{"slug":"/research-1/"},"frontmatter":{"date":"July 16, 2023","title":"Flash: Concept Drift Adaptation in Federated Learning","tags":["Federated Learning","Concept Drift","Drift Adaptation","Adaptive Optimization"],"authors":["Kunjal Panchal","Sunav Choudhary","Koyel Mukherjee","Subrata Mitra","Somdeb Sarkhel","Saayan Mitra","Hui Guan"],"venue":"ICML, 2023","period":"July 2023","url":"http://proceedings.mlr.press/v202/panchal23a/panchal23a.pdf","description":"In Federated Learning (FL), adaptive optimization is an effective approach to addressing the statistical heterogeneity issue but cannot adapt quickly to concept drifts. In this work, we propose a novel adaptive optimizer called Flash that simultaneously addresses both statistical heterogeneity and the concept drift issues.  The fundamental insight is that a concept drift can be detected based on the magnitude of parameter updates that are required to fit the global model to each participating client's local data distribution. Flash uses a two-pronged approach that synergizes client-side early-stopping  training to facilitate detection of concept drifts and the server-side drift-aware adaptive optimization to effectively adjust effective learning rate. We theoretically prove that Flash matches the convergence rate of state-of-the-art adaptive optimizers and further empirically evaluate the efficacy of Flash on a variety of FL benchmarks using different concept drift settings.  "}}},{"node":{"excerpt":"","fields":{"slug":"/research-2/"},"frontmatter":{"date":"June 16, 2021","title":"Poster: Flow: Fine-grained Personalized Federated Learning through Dynamic Routing","tags":["Federated Learning","Personalization","Dynamic Routing"],"authors":["Kunjal Panchal","Hui Guan"],"venue":"CrossFL Workshop @ MLSys, 2022","period":"September 2022","url":"https://crossfl2022.github.io/abstracts/Abstract9.pdf","description":"Personalization in Federated Learning (FL) has been proven effective for incentivizing clients to participate in the training. However, personalization has been only studied at a coarse granularity where all the input instances of a client (heterogeneous or otherwise) only use its individual local model, despite it being limited to only that client's data. Flow explores instance-level personalization through dynamically making routing decisions between the local and the global model, with the aim of achieving superior personalized performance for a given instance. Besides, as cross-device FL deals with millions of resource-constrained client devices, we push towards stateless personalization where a client doesn't need to carry its personalized state across FL rounds.  "}}}]}},"pageContext":{}},"staticQueryHashes":["3649515864","63159454"]}